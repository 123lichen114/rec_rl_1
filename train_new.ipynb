{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:28:56.604185: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 13:28:56.678110: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 13:28:57.416375: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "#Dependencies\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "from tensorflowonspark import TFCluster\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "from envs import OfflineEnv\n",
    "from recommender import DRRAgent\n",
    "\n",
    "import os\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/data2/lc/my_envs/rec_rl_1/bin/python3.8\"\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/data2/lc/my_envs/rec_rl_1/bin/python3.8\"\n",
    "ROOT_DIR = os.getcwd()\n",
    "DATA_DIR = os.path.join(ROOT_DIR, 'ml-1m')\n",
    "STATE_SIZE = 10\n",
    "MAX_EPISODE_NUM = 10\n",
    "\n",
    "def train_fn(argv,ctx):\n",
    "    # 初始化环境\n",
    "    print('Data loading...')\n",
    "    #Loading datasets\n",
    "    ratings_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'ratings.dat'), 'r').readlines()]\n",
    "    users_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'users.dat'), 'r').readlines()]\n",
    "    movies_list = [i.strip().split(\"::\") for i in open(os.path.join(DATA_DIR,'movies.dat'),encoding='latin-1').readlines()]\n",
    "    ratings_df = pd.DataFrame(ratings_list, columns = ['UserID', 'MovieID', 'Rating', 'Timestamp'], dtype = np.uint32)\n",
    "    movies_df = pd.DataFrame(movies_list, columns = ['MovieID', 'Title', 'Genres'])\n",
    "    movies_df['MovieID'] = movies_df['MovieID'].apply(pd.to_numeric)\n",
    "\n",
    "    print(\"Data loading complete!\")\n",
    "    print(\"Data preprocessing...\")\n",
    "\n",
    "    # 电影 id 到电影标题的映射\n",
    "    movies_id_to_movies = {movie[0]: movie[1:] for movie in movies_list}\n",
    "    ratings_df = ratings_df.applymap(int)\n",
    "\n",
    "    # 按用户整理看过的电影\n",
    "    users_dict = np.load('./data/user_dict.npy', allow_pickle=True)\n",
    "\n",
    "    # 每个用户的电影历史长度\n",
    "    users_history_lens = np.load('./data/users_histroy_len.npy')\n",
    "\n",
    "    users_num = max(ratings_df[\"UserID\"])+1\n",
    "    items_num = max(ratings_df[\"MovieID\"])+1\n",
    "\n",
    "    # 训练设置\n",
    "    train_users_num = int(users_num * 0.8)\n",
    "    train_items_num = items_num\n",
    "    train_users_dict = {k:users_dict.item().get(k) for k in range(1, train_users_num+1)}\n",
    "    train_users_history_lens = users_history_lens[:train_users_num]\n",
    "\n",
    "    print('DONE!')\n",
    "    time.sleep(2)\n",
    "\n",
    "    env = OfflineEnv(train_users_dict, train_users_history_lens, movies_id_to_movies, STATE_SIZE)\n",
    "    recommender = DRRAgent(env, users_num, items_num, STATE_SIZE, use_wandb=False)\n",
    "    recommender.actor.build_networks()\n",
    "    recommender.critic.build_networks()\n",
    "    recommender.train(MAX_EPISODE_NUM, load_model=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/03/26 13:29:12 WARN Utils: Your hostname, bit3090 resolves to a loopback address: 127.0.1.1; using 192.168.0.4 instead (on interface eno2np1)\n",
      "25/03/26 13:29:12 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/03/26 13:29:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# 创建 SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DistributedDRR\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 从 SparkSession 中获取 SparkContext\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# 配置 TensorFlow on Spark 集群\n",
    "tf_args = []  # 定义 tf_args 参数\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:29:23,101 INFO (MainThread-275056) Reserving TFSparkNodes \n",
      "2025-03-26 13:29:23,103 INFO (MainThread-275056) cluster_template: {'ps': [0], 'worker': [1, 2, 3]}\n",
      "2025-03-26 13:29:23,110 INFO (MainThread-275056) Reservation server binding to port 0\n",
      "2025-03-26 13:29:23,112 INFO (MainThread-275056) listening for reservations at ('192.168.0.4', 33299)\n",
      "2025-03-26 13:29:23,114 INFO (MainThread-275056) Starting TensorFlow on executors\n",
      "2025-03-26 13:29:23,422 INFO (MainThread-275056) Waiting for TFSparkNodes to start\n",
      "2025-03-26 13:29:23,426 INFO (MainThread-275056) waiting for 4 reservations\n",
      "2025-03-26 13:29:24,428 INFO (MainThread-275056) waiting for 4 reservations / 4]\n",
      "2025-03-26 13:29:25,430 INFO (MainThread-275056) waiting for 4 reservations\n",
      "2025-03-26 13:29:25.968768: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different 2025-03-26 13:29:25.968768: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 13:29:26.006531: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 13:29:26.012121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use avai2025-03-26 13:29:26.012121: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, ilable CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "n other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 13:29:26.048119: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 13:29:26.052350: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-26 13:29:26.094875: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-26 13:29:26,433 INFO (MainThread-275056) waiting for 4 reservations\n",
      "2025-03-26 13:29:26.683972: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-26 13:29:26.686919: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-26 13:29:26.703252: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-26 13:29:26.755985: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2025-03-26 13:29:27,435 INFO (MainThread-275056) waiting for 4 reservations\n",
      "2025-03-26 13:29:28,438 INFO (MainThread-275056) waiting for 4 reservations\n",
      "2025-03-26 13:29:29,032 INFO (MainThread-275604) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:29,032 INFO (MainThread-275604) Proposed GPUs: ['0']\n",
      "2025-03-26 13:29:29,032 INFO (MainThread-275604) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=0\n",
      "2025-03-26 13:29:29,064 INFO (MainThread-275604) connected to server at ('192.168.0.4', 33299)\n",
      "2025-03-26 13:29:29,066 INFO (MainThread-275604) TFSparkNode.reserve: {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:29,087 INFO (MainThread-275608) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:29,087 INFO (MainThread-275608) Proposed GPUs: ['0']\n",
      "2025-03-26 13:29:29,087 INFO (MainThread-275608) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=0\n",
      "2025-03-26 13:29:29,124 INFO (MainThread-275616) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:29,125 INFO (MainThread-275616) Proposed GPUs: ['0']\n",
      "2025-03-26 13:29:29,125 INFO (MainThread-275616) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=0\n",
      "2025-03-26 13:29:29,154 INFO (MainThread-275616) connected to server at ('192.168.0.4', 33299)\n",
      "2025-03-26 13:29:29,155 INFO (MainThread-275616) TFSparkNode.reserve: {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:29,193 INFO (MainThread-275605) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:29,194 INFO (MainThread-275605) Proposed GPUs: ['0']\n",
      "2025-03-26 13:29:29,194 INFO (MainThread-275605) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=0\n",
      "2025-03-26 13:29:29,216 INFO (MainThread-275605) connected to server at ('192.168.0.4', 33299)\n",
      "2025-03-26 13:29:29,217 INFO (MainThread-275605) TFSparkNode.reserve: {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:29,280 INFO (MainThread-275608) connected to server at ('192.168.0.4', 33299)\n",
      "2025-03-26 13:29:29,282 INFO (MainThread-275608) TFSparkNode.reserve: {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n",
      "2025-03-26 13:29:29,441 INFO (MainThread-275056) all reservations completed\n",
      "2025-03-26 13:29:29,443 INFO (MainThread-275056) All TFSparkNodes started\n",
      "2025-03-26 13:29:29,444 INFO (MainThread-275056) {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:29,445 INFO (MainThread-275056) {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:29,446 INFO (MainThread-275056) {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:29,447 INFO (MainThread-275056) {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-26 13:29:30,284 INFO (MainThread-275608) node: {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n",
      "2025-03-26 13:29:30,284 INFO (MainThread-275608) node: {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:30,284 INFO (MainThread-275608) node: {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:30,284 INFO (MainThread-275608) node: {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:30,652 INFO (MainThread-275608) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:30,653 INFO (MainThread-275608) Proposed GPUs: ['0']\n",
      "2025-03-26 13:29:30,653 INFO (MainThread-275608) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=0\n",
      "2025-03-26 13:29:30,654 INFO (MainThread-275608) Starting TensorFlow ps:0 as ps on cluster node 0 on background process\n",
      "Data loading...\n",
      "2025-03-26 13:29:31,070 INFO (MainThread-275604) node: {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n",
      "2025-03-26 13:29:31,070 INFO (MainThread-275604) node: {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:31,070 INFO (MainThread-275604) node: {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:31,070 INFO (MainThread-275604) node: {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:31,159 INFO (MainThread-275616) node: {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n",
      "2025-03-26 13:29:31,159 INFO (MainThread-275616) node: {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:31,160 INFO (MainThread-275616) node: {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:31,160 INFO (MainThread-275616) node: {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:31,221 INFO (MainThread-275605) node: {'executor_id': 0, 'host': '192.168.0.4', 'job_name': 'ps', 'task_index': 0, 'port': 43891, 'tb_pid': 0, 'tb_port': 0, 'addr': ('192.168.0.4', 38705), 'authkey': b'\\xd2\\xd7\\xae=eWF@\\xb8\\xd8\\xc4\\xf8N\\xe8F.'}\n",
      "2025-03-26 13:29:31,221 INFO (MainThread-275605) node: {'executor_id': 1, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 0, 'port': 35689, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-9tr43j3_/listener-pqypl1ku', 'authkey': b'j\\xd7\\xdf;H\\xe5A<\\x86k\\xaa\\x86\\xf8\\xe5\\xa9\\xd4'}\n",
      "2025-03-26 13:29:31,221 INFO (MainThread-275605) node: {'executor_id': 2, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 1, 'port': 37933, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-8v4kn4u2/listener-odn8do21', 'authkey': b'\\xff\\x8bv\\xfc\\xca\\x14Ed\\xa0\\xeb\\x92\\x8bd\\x87\\xd3F'}\n",
      "2025-03-26 13:29:31,221 INFO (MainThread-275605) node: {'executor_id': 3, 'host': '192.168.0.4', 'job_name': 'worker', 'task_index': 2, 'port': 44765, 'tb_pid': 0, 'tb_port': 0, 'addr': '/tmp/pymp-dpmjimko/listener-gczlhfi1', 'authkey': b'\\x9fT\\tMf\\xfcI\\xca\\xafP\\x15\\xd2f\\xe0\\xdf\\xb8'}\n",
      "2025-03-26 13:29:31,503 INFO (MainThread-275604) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:31,504 INFO (MainThread-275604) Proposed GPUs: ['3']\n",
      "2025-03-26 13:29:31,504 INFO (MainThread-275604) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=3\n",
      "2025-03-26 13:29:31,504 INFO (MainThread-275604) Starting TensorFlow worker:2 on cluster node 3 on foreground thread\n",
      "Data loading...\n",
      "2025-03-26 13:29:31,626 INFO (MainThread-275616) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:31,626 INFO (MainThread-275616) Proposed GPUs: ['2']\n",
      "2025-03-26 13:29:31,626 INFO (MainThread-275616) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=2\n",
      "2025-03-26 13:29:31,627 INFO (MainThread-275616) Starting TensorFlow worker:1 on cluster node 2 on foreground thread\n",
      "Data loading...\n",
      "2025-03-26 13:29:31,694 INFO (MainThread-275605) Available GPUs: ['0', '1', '2', '3', '4', '5', '6', '7']\n",
      "2025-03-26 13:29:31,695 INFO (MainThread-275605) Proposed GPUs: ['1']\n",
      "2025-03-26 13:29:31,695 INFO (MainThread-275605) Requested 1 GPU(s), setting CUDA_VISIBLE_DEVICES=1\n",
      "2025-03-26 13:29:31,695 INFO (MainThread-275605) Starting TensorFlow worker:0 on cluster node 1 on foreground thread\n",
      "Data loading...\n",
      "25/03/26 13:29:33 ERROR Executor: Exception in task 3.0 in stage 0.0 (TID 3)\n",
      "org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "25/03/26 13:29:33 WARN TaskSetManager: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "25/03/26 13:29:33 ERROR TaskSetManager: Task 3 in stage 0.0 failed 1 times; aborting job\n",
      "25/03/26 13:29:34 WARN TaskSetManager: Lost task 2.0 in stage 0.0 (TID 2) (192.168.0.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/26 13:29:34 WARN TaskSetManager: Lost task 0.0 in stage 0.0 (TID 0) (192.168.0.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "25/03/26 13:29:34 WARN TaskSetManager: Lost task 1.0 in stage 0.0 (TID 1) (192.168.0.4 executor driver): TaskKilled (Stage cancelled: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:)\n",
      "2025-03-26 13:29:34,214 ERROR (Thread-7-275056) Exception in TF background thread: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n",
      ": org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 0.0 failed 1 times, most recent failure: Lost task 3.0 in stage 0.0 (TID 3) (192.168.0.4 executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "\n",
      "Driver stacktrace:\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.failJobAndIndependentStages(DAGScheduler.scala:2856)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2(DAGScheduler.scala:2792)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$abortStage$2$adapted(DAGScheduler.scala:2791)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach(ResizableArray.scala:62)\n",
      "\tat scala.collection.mutable.ResizableArray.foreach$(ResizableArray.scala:55)\n",
      "\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:2791)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.$anonfun$handleTaskSetFailed$1$adapted(DAGScheduler.scala:1247)\n",
      "\tat scala.Option.foreach(Option.scala:407)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:1247)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:3060)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2994)\n",
      "\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:2983)\n",
      "\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:49)\n",
      "\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:989)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2393)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2414)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2458)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$1(RDD.scala:1049)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\n",
      "\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\n",
      "\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:410)\n",
      "\tat org.apache.spark.rdd.RDD.collect(RDD.scala:1048)\n",
      "\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:195)\n",
      "\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n",
      "\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n",
      "\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n",
      "\tat java.lang.reflect.Method.invoke(Method.java:498)\n",
      "\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n",
      "\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n",
      "\tat py4j.Gateway.invoke(Gateway.java:282)\n",
      "\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n",
      "\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n",
      "\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n",
      "\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n",
      "\tat java.lang.Thread.run(Thread.java:750)\n",
      "Caused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1247, in main\n",
      "    process()\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/python/lib/pyspark.zip/pyspark/worker.py\", line 1237, in process\n",
      "    out_iter = func(split_index, iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 5434, in pipeline_func\n",
      "    return func(split, prev_func(split, iterator))\n",
      "  [Previous line repeated 1 more time]\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 840, in func\n",
      "    return f(iterator)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pyspark/rdd.py\", line 1795, in func\n",
      "    r = f(it)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 462, in _mapfn\n",
      "    wrapper_fn(tf_args, ctx)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/tensorflowonspark/TFSparkNode.py\", line 421, in wrapper_fn\n",
      "    fn(args, context)\n",
      "  File \"/tmp/ipykernel_275056/3618200332.py\", line 29, in train_fn\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/frame.py\", line 790, in __init__\n",
      "    mgr = arrays_to_mgr(\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 120, in arrays_to_mgr\n",
      "    arrays, refs = _homogenize(arrays, index, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/internals/construction.py\", line 607, in _homogenize\n",
      "    val = sanitize_array(val, index, dtype=dtype, copy=False)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 576, in sanitize_array\n",
      "    subarr = _try_cast(data, dtype, copy)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/construction.py\", line 763, in _try_cast\n",
      "    subarr = maybe_cast_to_integer_array(arr, dtype)\n",
      "  File \"/data2/lc/my_envs/rec_rl_1/lib/python3.8/site-packages/pandas/core/dtypes/cast.py\", line 1680, in maybe_cast_to_integer_array\n",
      "    if is_unsigned_integer_dtype(dtype) and (arr < 0).any():\n",
      "TypeError: '<' not supported between instances of 'str' and 'int'\n",
      "\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.handlePythonException(PythonRunner.scala:572)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:784)\n",
      "\tat org.apache.spark.api.python.PythonRunner$$anon$3.read(PythonRunner.scala:766)\n",
      "\tat org.apache.spark.api.python.BasePythonRunner$ReaderIterator.hasNext(PythonRunner.scala:525)\n",
      "\tat org.apache.spark.InterruptibleIterator.hasNext(InterruptibleIterator.scala:37)\n",
      "\tat scala.collection.Iterator.foreach(Iterator.scala:943)\n",
      "\tat scala.collection.Iterator.foreach$(Iterator.scala:943)\n",
      "\tat org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq(Growable.scala:62)\n",
      "\tat scala.collection.generic.Growable.$plus$plus$eq$(Growable.scala:53)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:105)\n",
      "\tat scala.collection.mutable.ArrayBuffer.$plus$plus$eq(ArrayBuffer.scala:49)\n",
      "\tat scala.collection.TraversableOnce.to(TraversableOnce.scala:366)\n",
      "\tat scala.collection.TraversableOnce.to$(TraversableOnce.scala:364)\n",
      "\tat org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toBuffer(TraversableOnce.scala:358)\n",
      "\tat scala.collection.TraversableOnce.toBuffer$(TraversableOnce.scala:358)\n",
      "\tat org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)\n",
      "\tat scala.collection.TraversableOnce.toArray(TraversableOnce.scala:345)\n",
      "\tat scala.collection.TraversableOnce.toArray$(TraversableOnce.scala:339)\n",
      "\tat org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)\n",
      "\tat org.apache.spark.rdd.RDD.$anonfun$collect$2(RDD.scala:1049)\n",
      "\tat org.apache.spark.SparkContext.$anonfun$runJob$5(SparkContext.scala:2433)\n",
      "\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:93)\n",
      "\tat org.apache.spark.TaskContext.runTaskWithListeners(TaskContext.scala:166)\n",
      "\tat org.apache.spark.scheduler.Task.run(Task.scala:141)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.$anonfun$run$4(Executor.scala:620)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally(SparkErrorUtils.scala:64)\n",
      "\tat org.apache.spark.util.SparkErrorUtils.tryWithSafeFinally$(SparkErrorUtils.scala:61)\n",
      "\tat org.apache.spark.util.Utils$.tryWithSafeFinally(Utils.scala:94)\n",
      "\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:623)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n",
      "\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n",
      "\t... 1 more\n",
      "\n",
      "2025-03-26 13:29:34,260 INFO (Dummy-8-275056) Closing down clientserver connection\n"
     ]
    }
   ],
   "source": [
    "cluster = TFCluster.run(sc, train_fn, tf_args, num_executors=4, num_ps=1, tensorboard=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.shutdown()\n",
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
